#!/usr/bin/env python3
"""
Táº¡o data vá»›i nhiá»u toxic comments Ä‘á»ƒ test alerts (toxic_ratio > 0.7)
"""
import json
from datetime import datetime, timedelta
from pathlib import Path

def create_toxic_sample_data():
    """Táº¡o data vá»›i nhiá»u toxic comments Ä‘á»ƒ trigger alerts"""

    now = datetime.now()

    toxic_data = [
        {
            "video_id": "toxic_video_001",
            "caption": "Video gÃ¢y tranh cÃ£i",
            "comments": [
                {
                    "user_id": "user_toxic_001",
                    "text": "Äá»“ ngu ngá»‘c hÆ° há»ng",  # Toxic
                    "created_at": (now - timedelta(seconds=50)).strftime("%Y-%m-%dT%H:%M:%S")
                },
                {
                    "user_id": "user_toxic_002",
                    "text": "Tháº±ng khá»‘n náº¡n",  # Toxic
                    "created_at": (now - timedelta(seconds=40)).strftime("%Y-%m-%dT%H:%M:%S")
                },
                {
                    "user_id": "user_toxic_003",
                    "text": "Äá»“ Ã³c chÃ³ ngu si",  # Toxic
                    "created_at": (now - timedelta(seconds=30)).strftime("%Y-%m-%dT%H:%M:%S")
                },
                {
                    "user_id": "user_toxic_004",
                    "text": "Video nÃ y tá»‡ háº¡i",  # Toxic
                    "created_at": (now - timedelta(seconds=20)).strftime("%Y-%m-%dT%H:%M:%S")
                },
                {
                    "user_id": "user_clean_001",
                    "text": "TÃ´i thÃ­ch video nÃ y",  # Clean
                    "created_at": (now - timedelta(seconds=10)).strftime("%Y-%m-%dT%H:%M:%S")
                }
            ],
            "hashtags": ["controversial", "debate", "opinion"],
            "created_at": (now - timedelta(minutes=2)).strftime("%Y-%m-%dT%H:%M:%S")
        },
        {
            "video_id": "toxic_video_002",
            "caption": "Ná»™i dung gÃ¢y sá»‘c",
            "comments": [
                {
                    "user_id": "user_toxic_005",
                    "text": "Äá»“ rÃ¡c rÆ°á»Ÿi",  # Toxic
                    "created_at": (now - timedelta(seconds=45)).strftime("%Y-%m-%dT%H:%M:%S")
                },
                {
                    "user_id": "user_toxic_006",
                    "text": "Tháº­t kinh tá»Ÿm",  # Toxic
                    "created_at": (now - timedelta(seconds=35)).strftime("%Y-%m-%dT%H:%M:%S")
                },
                {
                    "user_id": "user_toxic_007",
                    "text": "Äá»“ khá»‘n kiáº¿p",  # Toxic
                    "created_at": (now - timedelta(seconds=25)).strftime("%Y-%m-%dT%H:%M:%S")
                },
                {
                    "user_id": "user_toxic_008",
                    "text": "TÃ´i ghÃ©t cÃ¡i nÃ y",  # Toxic
                    "created_at": (now - timedelta(seconds=15)).strftime("%Y-%m-%dT%H:%M:%S")
                },
                {
                    "user_id": "user_toxic_009",
                    "text": "Tháº­t vÃ´ lÃ½",  # Toxic
                    "created_at": (now - timedelta(seconds=5)).strftime("%Y-%m-%dT%H:%M:%S")
                }
            ],
            "hashtags": ["shocking", "controversy", "debate"],
            "created_at": (now - timedelta(minutes=1)).strftime("%Y-%m-%dT%H:%M:%S")
        }
    ]

    # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
    raw_dir = Path("data/raw")
    raw_dir.mkdir(parents=True, exist_ok=True)

    # Ghi file
    output_file = raw_dir / "toxic_test_data.jsonl"
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in toxic_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

    print(f"âœ… ÄÃ£ táº¡o data toxic: {output_file}")
    print(f"ðŸ“Š Timestamp hiá»‡n táº¡i: {now.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ðŸ“ Video 1: 5 comments (4 toxic = 80% toxic_ratio)")
    print(f"ðŸ“ Video 2: 5 comments (5 toxic = 100% toxic_ratio)")
    print(f"ðŸš¨ Dá»± kiáº¿n: Cáº£ 2 video sáº½ trigger alerts (toxic_ratio > 0.7)")

    return str(output_file)

if __name__ == "__main__":
    create_toxic_sample_data()
