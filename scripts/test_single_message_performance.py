#!/usr/bin/env python3
"""
Äá»c Táº¤T Cáº¢ messages tá»« Kafka vÃ  lÆ°u vÃ o PostgreSQL
"""
import sys
import time
import json
import yaml
from collections import defaultdict
from datetime import datetime, timedelta
from pathlib import Path

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

from kafka import KafkaConsumer
from src.models.toxicity_detector import ToxicityDetector
from src.utils.db_utils import PostgresHelper

def get_main_hashtag(hashtags_list, main_hashtags=None):
    """
    XÃ¡c Ä‘á»‹nh main hashtag tá»« danh sÃ¡ch hashtags.
    
    Logic:
    1. So sÃ¡nh hashtags tá»« dá»¯ liá»‡u vá»›i main_hashtags tá»« config (sau khi normalize)
    2. Normalize: bá» dáº¥u #, lowercase, trim spaces Ä‘á»ƒ so sÃ¡nh
    3. Náº¿u khá»›p, tráº£ vá» giÃ¡ trá»‹ Gá»C tá»« config (cÃ³ dáº¥u #) Ä‘á»ƒ lÆ°u vÃ o DB
    
    Args:
        hashtags_list: List hashtags tá»« dá»¯ liá»‡u (cÃ³ thá»ƒ cÃ³ hoáº·c khÃ´ng cÃ³ dáº¥u #)
        main_hashtags: List main hashtags tá»« config (cÃ³ dáº¥u #)
    
    Returns:
        main_hashtag tá»« config náº¿u khá»›p, None náº¿u khÃ´ng khá»›p
    """
    if not hashtags_list or not main_hashtags:
        return None
    
    # Äáº£m báº£o hashtags_list lÃ  list
    if not isinstance(hashtags_list, list):
        return None
    
    # Lá»c bá» cÃ¡c giÃ¡ trá»‹ None hoáº·c rá»—ng
    hashtags_list = [h for h in hashtags_list if h]
    if not hashtags_list:
        return None
    
    def normalize_tag(tag):
        """
        Chuáº©n hÃ³a hashtag Ä‘á»ƒ so sÃ¡nh:
        - Bá» dáº¥u # á»Ÿ Ä‘áº§u (náº¿u cÃ³)
        - Lowercase
        - Trim spaces
        """
        if not tag:
            return ""
        # Chuyá»ƒn sang string náº¿u khÃ´ng pháº£i
        tag_str = str(tag).strip()
        if not tag_str:
            return ""
        # Bá» dáº¥u # á»Ÿ Ä‘áº§u náº¿u cÃ³
        tag_str = tag_str.lstrip('#')
        # Lowercase vÃ  trim
        return tag_str.lower().strip()
    
    # Chuáº©n hÃ³a táº¥t cáº£ hashtags trong list
    normalized_hashtags = [normalize_tag(h) for h in hashtags_list]
    # Lá»c bá» cÃ¡c giÃ¡ trá»‹ rá»—ng sau khi normalize
    normalized_hashtags = [h for h in normalized_hashtags if h]
    
    if not normalized_hashtags:
        return None
    
    # So sÃ¡nh vá»›i main_hashtags tá»« config
    # QUAN TRá»ŒNG: Tráº£ vá» giÃ¡ trá»‹ Gá»C tá»« config (cÃ³ dáº¥u #) Ä‘á»ƒ lÆ°u vÃ o DB
    for main_tag in main_hashtags:
        if not main_tag:
            continue
        normalized_main = normalize_tag(main_tag)
        if normalized_main and normalized_main in normalized_hashtags:
            # Tráº£ vá» giÃ¡ trá»‹ gá»‘c tá»« config (cÃ³ dáº¥u #)
            return main_tag
    
    return None

def process_all_kafka_messages():
    """Äá»c vÃ  xá»­ lÃ½ Táº¤T Cáº¢ messages tá»« Kafka, lÆ°u vÃ o DB"""
    print("=" * 60)
    print("ğŸš€ Processing All Kafka Messages to Database")
    print("=" * 60)
    print()

    # 1. Äá»c Táº¤T Cáº¢ messages tá»« Kafka
    print("ğŸ“¥ Äang Ä‘á»c táº¥t cáº£ messages tá»« Kafka...")
    start_read = time.time()

    consumer = KafkaConsumer(
        "tiktok-raw",
        bootstrap_servers="127.0.0.1:9092",
        auto_offset_reset='earliest',
        enable_auto_commit=False,
        consumer_timeout_ms=10000  # TÄƒng timeout
    )

    all_messages = []
    message_count = 0

    try:
        for msg in consumer:
            try:
                data = json.loads(msg.value.decode('utf-8'))
                all_messages.append(data)
                message_count += 1

                # Hiá»ƒn thá»‹ progress má»—i 10 messages
                if message_count % 10 == 0:
                    print(f"   ğŸ“¦ ÄÃ£ Ä‘á»c {message_count} messages...")

            except json.JSONDecodeError:
                continue  # Bá» qua message khÃ´ng parse Ä‘Æ°á»£c

    except Exception as e:
        print(f"âš ï¸  Dá»«ng Ä‘á»c sau {message_count} messages: {e}")

    consumer.close()

    if not all_messages:
        print("âŒ KhÃ´ng cÃ³ message nÃ o trong Kafka topic!")
        print("   HÃ£y cháº¡y: python scripts/produce_test_message.py")
        return

    read_time = time.time() - start_read
    print(f"âœ“ Äá»c {message_count} messages thÃ nh cÃ´ng ({read_time:.2f}s)")
    print()
    
    # 2. Tá»•ng há»£p táº¥t cáº£ comments tá»« táº¥t cáº£ messages
    print("ğŸ“ Äang tá»•ng há»£p comments tá»« táº¥t cáº£ messages...")
    start_parse = time.time()

    all_comments = []
    all_comment_texts = []
    video_count = 0

    for data in all_messages:
        video_id = data.get('video_id', 'unknown')
        comments = data.get('comments', [])
        hashtags = data.get('hashtags', [])

        if comments:
            video_count += 1
            for comment in comments:
                text = comment.get('text', '').strip()
                if text:  # Chá»‰ láº¥y comments cÃ³ text
                    all_comments.append({
                        'video_id': video_id,
                        'user_id': comment.get('user_id', ''),
                        'text': text,
                        'hashtags': hashtags
                    })
                    all_comment_texts.append(text)

    parse_time = time.time() - start_parse
    print(f"âœ“ Tá»•ng há»£p xong ({parse_time:.3f}s)")
    print(f"   Videos: {video_count}")
    print(f"   Total comments: {len(all_comments)}")
    print()

    if not all_comment_texts:
        print("âŒ KhÃ´ng cÃ³ comment nÃ o Ä‘á»ƒ xá»­ lÃ½!")
        return

    # 3. Load model (láº§n Ä‘áº§u sáº½ cháº­m)
    print("ğŸ¤– Load model ViHateT5 (batch_size=32)...")
    start_model_load = time.time()

    detector = ToxicityDetector(
        model_name="tarudesu/ViHateT5-base-HSD",
        device="cpu",
        batch_size=32  # Giáº£m Ä‘á»ƒ nháº¹ hÆ¡n
    )

    model_load_time = time.time() - start_model_load
    print(f"âœ“ Model loaded ({model_load_time:.2f}s)")
    print()
    
    # 4. Xá»­ lÃ½ Táº¤T Cáº¢ comments vá»›i batch prediction
    print("=" * 60)
    print(f"ğŸ” Detect toxicity cho {len(all_comment_texts)} comments...")
    print("=" * 60)
    start_batch = time.time()

    # Process theo batches nhá» Ä‘á»ƒ trÃ¡nh memory issues
    batch_size = 100  # Process 100 comments cÃ¹ng lÃºc
    all_results = []

    for i in range(0, len(all_comment_texts), batch_size):
        batch_texts = all_comment_texts[i:i + batch_size]
        batch_results = detector.predict_batch(batch_texts)
        all_results.extend(batch_results)

        # Progress indicator
        processed = min(i + batch_size, len(all_comment_texts))
        print(f"   ğŸ“Š Processed {processed}/{len(all_comment_texts)} comments...")

    batch_total_time = time.time() - start_batch
    print()

    # Hiá»ƒn thá»‹ sample results
    print("ğŸ“‹ Sample Results:")
    for i, (comment, label) in enumerate(zip(all_comments[:10], all_results[:10]), 1):
        status = "ğŸš¨" if label in ["HATE", "OFFENSIVE"] else "âœ…"
        print(f"  {i}. {status} [{label:8s}] {comment['text'][:40]}...")

    if len(all_comments) > 10:
        print(f"  ... vÃ  {len(all_comments) - 10} comments khÃ¡c")

    print()
    print(f"âœ“ ÄÃ£ xá»­ lÃ½ {len(all_results)} comments trong {batch_total_time:.2f}s")
    print(f"  Trung bÃ¬nh: {batch_total_time/len(all_results)*1000:.1f}ms/comment")
    print(f"  Tá»‘c Ä‘á»™: {len(all_results)/batch_total_time:.1f} comments/sec")
    print()
    
    # Thá»‘ng kÃª toxicity tá»•ng thá»ƒ
    print("=" * 60)
    print("ğŸ“ˆ Thá»‘ng kÃª Toxicity Tá»•ng Thá»ƒ")
    print("=" * 60)
    toxic_count = sum(1 for r in all_results if r in ["HATE", "OFFENSIVE"])
    clean_count = sum(1 for r in all_results if r == "CLEAN")
    hate_count = sum(1 for r in all_results if r == "HATE")
    offensive_count = sum(1 for r in all_results if r == "OFFENSIVE")

    print(f"ğŸ“Š Tá»•ng káº¿t:")
    print(f"   Messages: {message_count}")
    print(f"   Videos: {video_count}")
    print(f"   Comments: {len(all_results)}")
    print()
    print(f"ğŸ¯ PhÃ¢n tÃ­ch Toxicity:")
    print(f"   CLEAN: {clean_count} ({clean_count/len(all_results)*100:.1f}%)")
    print(f"   OFFENSIVE: {offensive_count} ({offensive_count/len(all_results)*100:.1f}%)")
    print(f"   HATE: {hate_count} ({hate_count/len(all_results)*100:.1f}%)")
    print(f"   TOXIC (tá»•ng): {toxic_count} ({toxic_count/len(all_results)*100:.1f}%)")
    print()
    
    # 5. LÆ°u vÃ o PostgreSQL
    print("=" * 60)
    print("ğŸ’¾ Äang lÆ°u vÃ o PostgreSQL...")
    print("=" * 60)
    start_db = time.time()
    db_time = 0
    
    try:
        # Load config
        project_root = Path(__file__).parent.parent
        config_path = project_root / "config" / "config.yaml"
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        postgres_config = config['postgres']
        db_helper = PostgresHelper(postgres_config)
        db_helper.connect()
        
        # Äáº£m báº£o cá»™t main_hashtag tá»“n táº¡i trong cáº£ 2 báº£ng (thÃªm náº¿u chÆ°a cÃ³)
        try:
            conn = db_helper.connect()
            cursor = conn.cursor()
            
            # Kiá»ƒm tra vÃ  thÃªm vÃ o speed_comments
            cursor.execute("""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name = 'speed_comments' AND column_name = 'main_hashtag'
            """)
            result = cursor.fetchone()
            if not result:
                print("âš ï¸  Cá»™t 'main_hashtag' chÆ°a tá»“n táº¡i trong speed_comments, Ä‘ang thÃªm...")
                cursor.execute("ALTER TABLE speed_comments ADD COLUMN main_hashtag VARCHAR(200);")
                conn.commit()
                print("âœ“ ÄÃ£ thÃªm cá»™t 'main_hashtag' vÃ o báº£ng speed_comments")
            
            # Kiá»ƒm tra vÃ  thÃªm vÃ o speed_video_stats
            cursor.execute("""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name = 'speed_video_stats' AND column_name = 'main_hashtag'
            """)
            result = cursor.fetchone()
            if not result:
                print("âš ï¸  Cá»™t 'main_hashtag' chÆ°a tá»“n táº¡i trong speed_video_stats, Ä‘ang thÃªm...")
                cursor.execute("ALTER TABLE speed_video_stats ADD COLUMN main_hashtag VARCHAR(200);")
                conn.commit()
                print("âœ“ ÄÃ£ thÃªm cá»™t 'main_hashtag' vÃ o báº£ng speed_video_stats")
            
            cursor.close()
        except Exception as e:
            conn.rollback()
            print(f"âš ï¸  KhÃ´ng thá»ƒ thÃªm cá»™t main_hashtag: {e}")
            print("   CÃ³ thá»ƒ cá»™t Ä‘Ã£ tá»“n táº¡i hoáº·c cÃ³ lá»—i khÃ¡c")
        
        # Láº¥y main hashtags tá»« config
        main_hashtags = config.get('speed_layer', {}).get('main_hashtags', [])
        if not main_hashtags:
            print("âš ï¸  Cáº£nh bÃ¡o: KhÃ´ng tÃ¬m tháº¥y main_hashtags trong config!")
        else:
            print(f"ğŸ“‹ Main hashtags tá»« config ({len(main_hashtags)} hashtags):")
            for i, tag in enumerate(main_hashtags, 1):
                print(f"   {i}. {tag}")
        
        # Debug: Hiá»ƒn thá»‹ má»™t sá»‘ hashtags máº«u tá»« dá»¯ liá»‡u
        sample_hashtags = set()
        hashtags_by_video = {}
        for comment in all_comments[:20]:  # Xem nhiá»u hÆ¡n Ä‘á»ƒ cÃ³ Ä‘á»§ máº«u
            video_id = comment.get('video_id')
            hashtags = comment.get('hashtags', [])
            if hashtags:
                sample_hashtags.update(hashtags)
                if video_id not in hashtags_by_video:
                    hashtags_by_video[video_id] = hashtags
        
        if sample_hashtags:
            print(f"\nğŸ“‹ Sample hashtags tá»« dá»¯ liá»‡u ({len(sample_hashtags)} unique hashtags):")
            sample_list = list(sample_hashtags)[:15]
            for i, tag in enumerate(sample_list, 1):
                print(f"   {i}. {tag}")
            
            # Test matching vá»›i má»™t vÃ i vÃ­ dá»¥
            print(f"\nğŸ” Test matching logic:")
            for video_id, hashtags in list(hashtags_by_video.items())[:3]:
                test_main = get_main_hashtag(hashtags, main_hashtags)
                print(f"   Video {video_id}: hashtags={hashtags} -> main_hashtag={test_main}")
        else:
            print("âš ï¸  KhÃ´ng tÃ¬m tháº¥y hashtags trong dá»¯ liá»‡u!")
        
        # Chuáº©n bá»‹ data Ä‘á»ƒ insert comments
        kafka_timestamp = datetime.now()  # DÃ¹ng thá»i gian hiá»‡n táº¡i vÃ¬ khÃ´ng cÃ³ message timestamp
        
        insert_query = """
        INSERT INTO speed_comments 
        (video_id, user_id, comment_text, toxicity_label, is_toxic, is_hate, is_offensive, is_clean, hashtags, main_hashtag, kafka_timestamp, processed_at)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        
        insert_data = []
        main_hashtag_count = 0
        debug_samples_shown = 0
        
        print(f"\nğŸ’¾ Äang chuáº©n bá»‹ insert {len(all_comments)} comments...")
        
        for comment, label in zip(all_comments, all_results):
            if not comment.get('text'):
                continue
            
            hashtags = comment.get('hashtags', [])
            main_hashtag = get_main_hashtag(hashtags, main_hashtags)
            
            # Debug: Hiá»ƒn thá»‹ má»™t vÃ i vÃ­ dá»¥ cá»¥ thá»ƒ
            if debug_samples_shown < 3:
                print(f"   ğŸ” VÃ­ dá»¥ {debug_samples_shown + 1}: hashtags={hashtags} -> main_hashtag='{main_hashtag}'")
                debug_samples_shown += 1
            
            if main_hashtag:
                main_hashtag_count += 1
                
            is_toxic = 1 if label in ["HATE", "OFFENSIVE"] else 0
            is_hate = 1 if label == "HATE" else 0
            is_offensive = 1 if label == "OFFENSIVE" else 0
            is_clean = 1 if label == "CLEAN" else 0
            
            insert_data.append((
                comment.get('video_id'),
                comment.get('user_id'),
                comment.get('text'),
                label,
                is_toxic,
                is_hate,
                is_offensive,
                is_clean,
                hashtags,
                main_hashtag,
                kafka_timestamp,
                datetime.now()
            ))
        
        # Batch insert comments
        db_helper.execute_many(insert_query, insert_data)
        print(f"âœ“ ÄÃ£ lÆ°u {len(insert_data)} comments vÃ o speed_comments")
        print(f"   ğŸ“Š Comments cÃ³ main_hashtag: {main_hashtag_count}/{len(insert_data)} ({main_hashtag_count/len(insert_data)*100:.1f}%)")
        
        # 6. TÃ­nh toÃ¡n vÃ  lÆ°u stats theo tá»«ng video vÃ o speed_video_stats
        print("ğŸ“Š Äang tÃ­nh toÃ¡n stats vÃ  lÆ°u vÃ o speed_video_stats...")
        
        # NhÃ³m comments theo video_id
        video_stats = defaultdict(lambda: {'comments': [], 'results': [], 'hashtags': []})
        
        for comment, label in zip(all_comments, all_results):
            video_id = comment.get('video_id')
            video_stats[video_id]['comments'].append(comment)
            video_stats[video_id]['results'].append(label)
            if comment.get('hashtags'):
                video_stats[video_id]['hashtags'] = comment.get('hashtags')
        
        # Táº¡o window (dÃ¹ng kafka_timestamp lÃ m window_end, window_start = window_end - 1 minute)
        window_end = kafka_timestamp
        window_start = window_end - timedelta(minutes=1)
        
        # Insert stats cho tá»«ng video
        stats_query = """
        INSERT INTO speed_video_stats 
        (video_id, window_start, window_end, total_comments, toxic_comments, hate_comments, offensive_comments, clean_comments, toxic_ratio, hashtags, main_hashtag)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        ON CONFLICT (video_id, window_end) 
        DO UPDATE SET
            total_comments = EXCLUDED.total_comments,
            toxic_comments = EXCLUDED.toxic_comments,
            hate_comments = EXCLUDED.hate_comments,
            offensive_comments = EXCLUDED.offensive_comments,
            clean_comments = EXCLUDED.clean_comments,
            toxic_ratio = EXCLUDED.toxic_ratio,
            hashtags = EXCLUDED.hashtags,
            main_hashtag = EXCLUDED.main_hashtag
        """
        
        stats_inserted = 0
        main_hashtag_stats_count = 0
        
        for video_id, stats in video_stats.items():
            results = stats['results']
            total_comments = len(results)
            toxic_comments = sum(1 for r in results if r in ["HATE", "OFFENSIVE"])
            hate_comments = sum(1 for r in results if r == "HATE")
            offensive_comments = sum(1 for r in results if r == "OFFENSIVE")
            clean_comments = sum(1 for r in results if r == "CLEAN")
            toxic_ratio = toxic_comments / total_comments if total_comments > 0 else 0.0
            hashtags = stats.get('hashtags', [])
            
            # TÃ­nh toÃ¡n main_hashtag tá»« hashtags cá»§a video
            main_hashtag = get_main_hashtag(hashtags, main_hashtags)
            if main_hashtag:
                main_hashtag_stats_count += 1
            
            stats_data = (
                video_id,
                window_start,
                window_end,
                total_comments,
                toxic_comments,
                hate_comments,
                offensive_comments,
                clean_comments,
                toxic_ratio,
                hashtags,
                main_hashtag
            )
            
            db_helper.execute_query(stats_query, stats_data)
            stats_inserted += 1
        
        db_time = time.time() - start_db
        print(f"âœ“ ÄÃ£ lÆ°u stats cho {stats_inserted} videos vÃ o speed_video_stats ({db_time:.2f}s)")
        print(f"   ğŸ“Š Videos cÃ³ main_hashtag: {main_hashtag_stats_count}/{stats_inserted} ({main_hashtag_stats_count/stats_inserted*100:.1f}%)")
        print(f"   Window: {window_start} -> {window_end}")
        print()
        
        db_helper.close()
        
    except Exception as e:
        print(f"âŒ Lá»—i khi lÆ°u vÃ o PostgreSQL: {e}")
        import traceback
        traceback.print_exc()
        print()
    
    print("=" * 60)
    print("ğŸ‰ Test hoÃ n thÃ nh!")
    print("=" * 60)
    print()

    total_time = read_time + model_load_time + batch_total_time
    print("ğŸ“Š Tá»”NG Káº¾T PERFORMANCE:")
    print(f"  ğŸ“¥ Äá»c {message_count} messages: {read_time:.2f}s")
    print(f"  ğŸ¤– Load model: {model_load_time:.2f}s")
    print(f"  ğŸš€ Xá»­ lÃ½ {len(all_results)} comments: {batch_total_time:.2f}s")
    print(f"  â±ï¸  Tá»”NG THá»œI GIAN: {total_time:.2f}s")
    print()
    print("ğŸ’¡ Hiá»‡u suáº¥t:")
    print(f"   - Tá»‘c Ä‘á»™ Ä‘á»c: {message_count/read_time:.1f} messages/sec")
    print(f"   - Tá»‘c Ä‘á»™ xá»­ lÃ½: {len(all_results)/batch_total_time:.1f} comments/sec")
    print(f"   - Trung bÃ¬nh: {batch_total_time/len(all_results)*1000:.1f}ms/comment")
    print()
    print("ğŸ¯ Káº¿t quáº£ phÃ¢n tÃ­ch:")
    print(f"   - Tá»•ng comments: {len(all_results)}")
    print(f"   - Tá»· lá»‡ toxic: {toxic_count/len(all_results)*100:.1f}%")
    print(f"   - Videos processed: {video_count}")
    print()

if __name__ == "__main__":
    try:
        process_all_kafka_messages()
    except KeyboardInterrupt:
        print("\nâš ï¸  ÄÃ£ dá»«ng processing")
    except Exception as e:
        print(f"\nâŒ Lá»—i: {e}")
        import traceback
        traceback.print_exc()
