"""
Toxicity Detection Model - Wrapper cho ViHateT5 models
"""
import logging
import warnings
from typing import List, Union

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Suppress HuggingFace deprecation warnings
warnings.filterwarnings("ignore", category=FutureWarning, module="huggingface_hub")

logger = logging.getLogger(__name__)


class ToxicityDetector:
    """
    Model Ä‘á»ƒ phÃ¡t hiá»‡n hate speech/toxicity trong vÄƒn báº£n tiáº¿ng Viá»‡t
    Sá»­ dá»¥ng cÃ¡c model tá»« series ViHateT5 cá»§a tarudesu
    """
    
    # Mapping labels
    LABEL_MAPPING = {
        "HATE": "HATE",
        "OFFENSIVE": "OFFENSIVE", 
        "CLEAN": "CLEAN"
    }
    
    def __init__(
        self, 
        model_name: str = "tarudesu/ViHateT5-HSD",
        device: str = "cpu",
        max_length: int = 256,
        batch_size: int = 32
    ):
        """
        Khá»Ÿi táº¡o model
        
        Args:
            model_name: TÃªn model trÃªn HuggingFace
            device: Device Ä‘á»ƒ cháº¡y model (cpu/cuda)
            max_length: Äá»™ dÃ i tá»‘i Ä‘a cá»§a input
            batch_size: Batch size khi predict nhiá»u text
        """
        self.model_name = model_name
        self.device = device
        self.max_length = max_length
        self.batch_size = batch_size

        # Detect if this is base model or fine-tuned model
        self.is_base_model = "base" in model_name and "HSD" not in model_name
        # Simple zero-shot prompting for base model
        self.task_prefix = "classify this text as CLEAN, OFFENSIVE, or HATE: " if self.is_base_model else ""
        
        logger.info(f"Loading model: {model_name}")
        
        try:
            # Load tokenizer vÃ  model
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
            
            # Move model to device
            if device == "cuda" and torch.cuda.is_available():
                self.model = self.model.cuda()
                logger.info("âœ“ Model loaded on GPU")
            else:
                self.model = self.model.cpu()
                logger.info("âœ“ Model loaded on CPU")
            
            self.model.eval()  # Set to evaluation mode
            
            logger.info(f"âœ“ ToxicityDetector initialized successfully")
        
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            raise
    
    def _decode_prediction(self, generated_ids) -> str:
        """
        Decode prediction tá»« model output
        
        Args:
            generated_ids: Output IDs tá»« model
            
        Returns:
            Label: HATE, OFFENSIVE, hoáº·c CLEAN
        """
        decoded = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        label = decoded.strip().upper()
        
        # Normalize label
        if "HATE" in label:
            return "HATE"
        elif "OFFENSIVE" in label or "OFFENS" in label:
            return "OFFENSIVE"
        else:
            return "CLEAN"
    
    def predict(self, text: str) -> str:
        """
        Dá»± Ä‘oÃ¡n toxicity cho má»™t vÄƒn báº£n
        
        Args:
            text: VÄƒn báº£n cáº§n phÃ¢n tÃ­ch
            
        Returns:
            Label: HATE, OFFENSIVE, hoáº·c CLEAN
        """
        if not text or not text.strip():
            return "CLEAN"
        
        try:
            # Add task prefix for base model
            input_text = self.task_prefix + text

            # Tokenize
            inputs = self.tokenizer(
                input_text,
                max_length=self.max_length,
                padding="max_length",
                truncation=True,
                return_tensors="pt"
            )
            
            # Move to device
            if self.device == "cuda":
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            # Generate prediction
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=10,
                    num_beams=1
                )
            
            # Decode
            label = self._decode_prediction(outputs)
            return label
        
        except Exception as e:
            logger.error(f"Prediction error: {e}")
            return "CLEAN"  # Default fallback
    
    def predict_batch(self, texts: List[str]) -> List[str]:
        """
        Dá»± Ä‘oÃ¡n toxicity cho nhiá»u vÄƒn báº£n (batch processing)
        
        Args:
            texts: Danh sÃ¡ch vÄƒn báº£n cáº§n phÃ¢n tÃ­ch
            
        Returns:
            Danh sÃ¡ch labels tÆ°Æ¡ng á»©ng
        """
        if not texts:
            return []
        
        results = []
        
        # Process in batches
        for i in range(0, len(texts), self.batch_size):
            batch_texts = texts[i:i + self.batch_size]
            batch_results = self._predict_batch_internal(batch_texts)
            results.extend(batch_results)
        
        return results
    
    def _predict_batch_internal(self, texts: List[str]) -> List[str]:
        """
        Xá»­ lÃ½ má»™t batch vÄƒn báº£n
        
        Args:
            texts: Batch vÄƒn báº£n
            
        Returns:
            Batch labels
        """
        # Replace empty texts
        texts = [t if t and t.strip() else " " for t in texts]
        
        try:
            # Tokenize batch
            inputs = self.tokenizer(
                texts,
                max_length=self.max_length,
                padding="max_length",
                truncation=True,
                return_tensors="pt"
            )
            
            # Move to device
            if self.device == "cuda":
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            # Generate predictions
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=10,
                    num_beams=1
                )
            
            # Decode all predictions
            labels = []
            for output in outputs:
                label = self._decode_prediction([output])
                labels.append(label)
            
            return labels
        
        except Exception as e:
            logger.error(f"Batch prediction error: {e}")
            return ["CLEAN"] * len(texts)
    
    def is_toxic(self, text: str) -> bool:
        """
        Kiá»ƒm tra xem vÄƒn báº£n cÃ³ toxic khÃ´ng
        
        Args:
            text: VÄƒn báº£n cáº§n kiá»ƒm tra
            
        Returns:
            True náº¿u toxic (HATE hoáº·c OFFENSIVE)
        """
        label = self.predict(text)
        return label in ["HATE", "OFFENSIVE"]
    
    def get_toxicity_score(self, text: str) -> float:
        """
        TÃ­nh Ä‘iá»ƒm toxicity (0-1)
        
        Args:
            text: VÄƒn báº£n cáº§n phÃ¢n tÃ­ch
            
        Returns:
            Score: 0.0 (CLEAN), 0.5 (OFFENSIVE), 1.0 (HATE)
        """
        label = self.predict(text)
        
        score_map = {
            "CLEAN": 0.0,
            "OFFENSIVE": 0.5,
            "HATE": 1.0
        }
        
        return score_map.get(label, 0.0)


def create_spark_udf(model_name: str = "tarudesu/ViHateT5-HSD", device: str = "cpu", batch_size: int = 32):
    """
    Táº¡o Spark Pandas UDF Ä‘á»ƒ xá»­ lÃ½ theo batch (nhanh hÆ¡n nhiá»u so vá»›i UDF thÃ´ng thÆ°á»ng)
    
    Args:
        model_name: TÃªn model
        device: Device
        batch_size: Batch size cho pandas UDF
        
    Returns:
        Spark Pandas UDF function
    """
    from pyspark.sql.functions import pandas_udf
    from pyspark.sql.types import StringType
    import pandas as pd
    
    # Khá»Ÿi táº¡o model (sáº½ Ä‘Æ°á»£c cache trÃªn má»—i executor)
    detector = None
    
    @pandas_udf(StringType())
    def predict_batch_udf(texts: pd.Series) -> pd.Series:
        """
        Xá»­ lÃ½ batch comments cÃ¹ng lÃºc thay vÃ¬ tá»«ng cÃ¡i má»™t
        """
        nonlocal detector
        if detector is None:
            logger.info(f"ðŸ”„ Initializing ToxicityDetector on executor (model: {model_name}, device: {device})")
            detector = ToxicityDetector(model_name=model_name, device=device, batch_size=batch_size)
            logger.info("âœ“ ToxicityDetector initialized on executor")
        
        # Convert pandas Series to list
        texts_list = texts.tolist()
        
        # Xá»­ lÃ½ theo batch Ä‘á»ƒ táº­n dá»¥ng batch processing cá»§a model
        results = detector.predict_batch(texts_list)
        
        # Tráº£ vá» pandas Series
        return pd.Series(results)
    
    return predict_batch_udf


if __name__ == "__main__":
    # Test
    logging.basicConfig(level=logging.INFO)
    
    detector = ToxicityDetector()
    
    # Test cases
    test_texts = [
        "Xin chÃ o, báº¡n khá»e khÃ´ng?",
        "Äá»“ ngu ngá»‘c!",
        "ChÃºng mÃ y lÃ  lÅ© khá»‘n náº¡n",
    ]
    
    print("\n=== Testing Toxicity Detector ===")
    for text in test_texts:
        label = detector.predict(text)
        score = detector.get_toxicity_score(text)
        print(f"Text: {text}")
        print(f"Label: {label}, Score: {score}\n")
    
    # Test batch
    print("\n=== Testing Batch Prediction ===")
    labels = detector.predict_batch(test_texts)
    for text, label in zip(test_texts, labels):
        print(f"{text} -> {label}")

